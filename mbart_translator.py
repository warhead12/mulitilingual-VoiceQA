# -*- coding: utf-8 -*-
"""mbart_translator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NUPb8KZSOlej8yXGkcalyf-jCm7_CpHI
"""

# !pip install transformers seqeval[gpu]
import torch
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
device = torch.device('cuda') if torch.cuda.is_available() else "cpu"
model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
model.to(device)


def translate_text(text, target_language_code):
    model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
    device = torch.device('cuda') if torch.cuda.is_available() else "cpu"
   #  model.to(device)

    tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
    
    # Set the source language to English
    tokenizer.src_lang = "en_XX"
    
    # Encode the input text
    encoded_text = tokenizer(text, return_tensors="pt")
    
    # Translate to the target language
    generated_tokens = model.generate(
        **encoded_text,
        forced_bos_token_id=tokenizer.lang_code_to_id[target_language_code]
    )
    
    # Decode and return the translated text
    translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
    
    return translated_text[0]

# Example usage:
# english_text = "The Secretary-General of the United Nations says there is no military solution in Syria"# 
# target_language_code = "hi_IN"  # Change to your desired target language code
# translated_text = translate_text(english_text, target_language_code)
# print(translated_text)

